---
- name: Uninstall Ceph/Rook cluster
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Ping host
      ping:

    - name: Delete all PVCs using Ceph storage
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🔍 Finding PVCs using Ceph storage..."
        
        # Get all PVCs using ceph-rbd storage class
        CEPH_PVCS=$(kubectl get pvc --all-namespaces -o json | jq -r '.items[] | select(.spec.storageClassName == "ceph-rbd") | "\(.metadata.namespace)/\(.metadata.name)"' || true)
        
        if [ -n "$CEPH_PVCS" ]; then
          echo "📋 Found Ceph PVCs to delete:"
          echo "$CEPH_PVCS"
          echo ""
          
          # Delete each PVC
          echo "$CEPH_PVCS" | while read pvc; do
            if [ -n "$pvc" ]; then
              namespace=$(echo "$pvc" | cut -d'/' -f1)
              name=$(echo "$pvc" | cut -d'/' -f2)
              echo "🗑️  Deleting PVC: $name in namespace $namespace"
              kubectl delete pvc "$name" -n "$namespace" --ignore-not-found=true
            fi
          done
          
          echo "⏳ Waiting for PVCs to be deleted..."
          sleep 30
        else
          echo "✅ No Ceph PVCs found to delete"
        fi
      ignore_errors: true

    - name: Remove Ceph StorageClass
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🗑️  Removing Ceph StorageClass..."
        kubectl delete storageclass ceph-rbd --ignore-not-found=true
        
        echo "🔄 Restoring local-path as default StorageClass..."
        kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' || true
      ignore_errors: true

    - name: Delete CephBlockPool
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🗑️  Deleting CephBlockPool..."
        kubectl delete cephblockpool replicapool -n rook-ceph --ignore-not-found=true
        
        echo "⏳ Waiting for pool deletion..."
        sleep 10
      ignore_errors: true

    - name: Delete CephCluster
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🗑️  Deleting CephCluster..."
        kubectl delete cephcluster rook-ceph -n rook-ceph --ignore-not-found=true
        
        echo "⏳ Waiting for cluster deletion (this may take several minutes)..."
        
        # Wait for OSDs to be terminated
        for i in {1..60}; do
          OSD_COUNT=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd --no-headers 2>/dev/null | wc -l || echo "0")
          if [ "$OSD_COUNT" -eq 0 ]; then
            echo "✅ All OSDs have been terminated"
            break
          fi
          echo "   Waiting for $OSD_COUNT OSDs to terminate..."
          sleep 10
        done
      ignore_errors: true

    - name: Uninstall Rook operator
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🗑️  Uninstalling Rook operator..."
        helm uninstall rook -n rook-ceph --ignore-not-found || true
        
        echo "⏳ Waiting for operator to be removed..."
        sleep 30
        
        echo "🗑️  Removing rook-ceph namespace..."
        kubectl delete namespace rook-ceph --ignore-not-found=true
        
        echo "⏳ Waiting for namespace deletion..."
        for i in {1..30}; do
          if ! kubectl get namespace rook-ceph 2>/dev/null; then
            echo "✅ Namespace rook-ceph deleted"
            break
          fi
          echo "   Waiting for namespace deletion..."
          sleep 5
        done
      ignore_errors: true

- name: Clean up storage devices on all nodes
  become: true
  vars:
    ceph_partition: "/dev/nvme0n1p3"
  hosts: [bootstrapMaster, masters, workers]
  tasks:
    - name: Clean Rook data directory
      shell: |-
        set -euxo pipefail
        
        echo "🧹 Cleaning Rook data directory..."
        
        # Remove Rook data directory contents
        if [ -d "/var/lib/rook" ]; then
          echo "   Removing /var/lib/rook contents..."
          rm -rf /var/lib/rook/* || true
          rm -rf /var/lib/rook/.* 2>/dev/null || true
        fi
        
        echo "✅ Rook data directory cleaned"
      ignore_errors: true

    - name: Wipe Ceph partition
      shell: |-
        set -euxo pipefail
        
        echo "🧹 Wiping Ceph partition {{ ceph_partition }}..."
        
        # Check if partition exists
        if [ -b "{{ ceph_partition }}" ]; then
          # Stop any processes using the partition
          fuser -k {{ ceph_partition }} || true
          
          # Wipe filesystem signatures
          wipefs -af {{ ceph_partition }} || true
          
          # Zero out the beginning of the partition
          dd if=/dev/zero of={{ ceph_partition }} bs=1M count=100 status=progress || true
          
          echo "✅ Ceph partition {{ ceph_partition }} wiped"
        else
          echo "⚠️  Ceph partition {{ ceph_partition }} not found, skipping"
        fi
      ignore_errors: true

    - name: Remove LVM volumes (if any)
      shell: |-
        set -euxo pipefail
        
        echo "🧹 Removing any Ceph LVM volumes..."
        
        # Remove any Ceph-related volume groups
        for vg in $(vgs --noheadings -o vg_name 2>/dev/null | grep -E '(ceph|rook)' || true); do
          echo "   Removing VG: $vg"
          vgremove -f "$vg" || true
        done
        
        # Remove any physical volumes on the Ceph partition
        if [ -b "{{ ceph_partition }}" ]; then
          pvremove -f "{{ ceph_partition }}" 2>/dev/null || true
        fi
        
        echo "✅ LVM cleanup completed"
      ignore_errors: true

    - name: Display cleanup status
      shell: |-
        echo "📊 Storage cleanup status:"
        echo ""
        echo "=== Partition status ==="
        lsblk {{ ceph_partition }} 2>/dev/null || echo "Partition not found"
        echo ""
        echo "=== Filesystem check ==="
        blkid {{ ceph_partition }} 2>/dev/null || echo "No filesystem detected (clean)"
        echo ""
        echo "=== Rook directory status ==="
        ls -la /var/lib/rook/ 2>/dev/null || echo "Directory empty or not found"
      register: cleanup_status
      ignore_errors: true

    - name: Show cleanup results
      debug:
        msg: "{{ cleanup_status.stdout_lines }}"
      when: cleanup_status.stdout_lines is defined

- name: Final cleanup verification
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Verify Rook/Ceph components are removed
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🔍 Verifying cleanup..."
        
        # Check for any remaining Rook/Ceph resources
        echo "=== Checking for remaining namespaces ==="
        kubectl get namespaces | grep -E '(rook|ceph)' || echo "No Rook/Ceph namespaces found ✅"
        echo ""
        
        echo "=== Checking for remaining storage classes ==="
        kubectl get storageclass | grep -E '(rook|ceph)' || echo "No Rook/Ceph storage classes found ✅"
        echo ""
        
        echo "=== Checking default storage class ==="
        kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}'
        echo ""
        echo ""
        
        echo "=== Checking for remaining PVs ==="
        kubectl get pv | grep -E '(rook|ceph)' || echo "No Rook/Ceph PVs found ✅"
        echo ""
        
        echo "🎉 Ceph/Rook cleanup verification completed!"
      register: verification_result
      ignore_errors: true

    - name: Show verification results
      debug:
        msg: "{{ verification_result.stdout_lines }}"
      when: verification_result.stdout_lines is defined

    - name: Display final cleanup summary
      debug:
        msg:
          - "=================================================="
          - "🎉 Ceph/Rook Cluster Cleanup Completed"
          - "=================================================="
          - ""
          - "✅ All PVCs using Ceph storage deleted"
          - "✅ CephBlockPool removed"
          - "✅ CephCluster deleted"
          - "✅ Rook operator uninstalled"
          - "✅ rook-ceph namespace removed"
          - "✅ Ceph storage devices wiped"
          - "✅ Rook data directories cleaned"
          - "✅ local-path restored as default StorageClass"
          - ""
          - "📋 Next Steps:"
          - "  • Storage devices are now clean and ready for reuse"
          - "  • You can run the install playbook again if needed"
          - "  • Consider running nvme-partitioning if you need to repartition"
