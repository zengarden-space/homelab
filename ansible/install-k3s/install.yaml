- name: System setup
  become: true
  hosts: pies
  tasks:
    - name: Ping hosts
      ping:
    - name: Update apt packages
      command: sudo apt-get update
    - name: Check if kernel parameters set
      stat:
        path: /etc/sysctl.d/90-kubelet.conf
      register: kubeletConfExists
    - name: kubelet kernel parameters
      shell: |-
        set -euxo pipefail
        tee /etc/sysctl.d/90-kubelet.conf <<'EOF'
        vm.panic_on_oom=0
        vm.overcommit_memory=1
        kernel.panic=10
        kernel.panic_on_oops=1
        EOF
      when: "not kubeletConfExists.stat.exists"
    - name: Check if cgroups are enabled
      command: cat /boot/firmware/cmdline.txt
      register: cmdlineContent
    - name: Enable cgroups
      command: sed -i -e 's/$/ cgroup_memory=1 cgroup_enable=memory/' /boot/firmware/cmdline.txt
      when: "'cgroup_memory=1 cgroup_enable=memory' not in cmdlineContent.stdout"
      notify:
        - Restart pi
  handlers:
    - name: Restart pi
      reboot:

- name: Prep servers
  become: true
  hosts: [bootstrapMaster, masters]
  tasks:
    - name: Ensure directory /var/lib/rancher/k3s/server exists
      file:
        path: /var/lib/rancher/k3s/server
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Ensure directory /etc/rancher exists
      file:
        path: /etc/rancher
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Ensure directory /etc/rancher/k3s exists
      file:
        path: /etc/rancher/k3s
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Put PSA policy
      copy:
        content: |-
          apiVersion: apiserver.config.k8s.io/v1
          kind: AdmissionConfiguration
          plugins:
          - name: PodSecurity
            configuration:
              apiVersion: pod-security.admission.config.k8s.io/v1beta1
              kind: PodSecurityConfiguration
              defaults:
                enforce: "restricted"
                enforce-version: "latest"
                audit: "restricted"
                audit-version: "latest"
                warn: "restricted"
                warn-version: "latest"
              exemptions:
                usernames: []
                runtimeClasses: []
                namespaces: [kube-system, rook-ceph, velero, metallb-system, victoria-metrics, gitea]
          - name: EventRateLimit
            configuration:
              apiVersion: eventratelimit.admission.k8s.io/v1alpha1
              kind: Configuration
              limits:
                - type: Server
                  qps: 50
                  burst: 100
                - type: Namespace
                  qps: 10
                  burst: 20
                - type: User
                  qps: 20
                  burst: 30
        dest: /var/lib/rancher/k3s/server/psa.yaml
      notify:
        - Restart k3s
    - name: Put audit policy
      copy:
        content: |-
          apiVersion: audit.k8s.io/v1
          kind: Policy
          rules:
          - level: Metadata
        dest: /var/lib/rancher/k3s/server/audit.yaml
      notify:
        - Restart k3s
    - name: Put k3s config
      copy:
        content: |-
          # Data directory configuration (should use NVMe mount)
          data-dir: "/var/lib/rancher/k3s"
          write-kubeconfig-mode: "600"
          flannel-backend: "none"
          disable-network-policy: true
          disable:
            - servicelb
            - traefik
          cluster-cidr: "10.42.0.0/16"
          service-cidr: "10.43.0.0/16"
          # TLS Configuration - Include node IPs and service IPs in certificates
          tls-san:
            - "{{ ansible_default_ipv4.address }}"
            - "{{ inventory_hostname }}"
            - "{{ inventory_hostname }}.{{ domain }}"
            - "10.43.0.1"  # Kubernetes service IP
            - "127.0.0.1"  # Localhost
            - "localhost"
            - "kubernetes"
            - "kubernetes.default"
            - "kubernetes.default.svc"
            - "kubernetes.default.svc.cluster.local"
            - "kubernetes.{{ domain }}"  # External API endpoint
            - "kubernetes.homelab.int.{{ domain }}"  # External API endpoint
          node-external-ip: "{{ ansible_default_ipv4.address }}"
          # Hardened security
          protect-kernel-defaults: true
          secrets-encryption: true
          kube-apiserver-arg:
            - "enable-admission-plugins=NodeRestriction,EventRateLimit"
            - 'admission-control-config-file=/var/lib/rancher/k3s/server/psa.yaml'
            - 'audit-log-path=/var/lib/rancher/k3s/server/logs/audit.log'
            - 'audit-policy-file=/var/lib/rancher/k3s/server/audit.yaml'
            - 'audit-log-maxage=30'
            - 'audit-log-maxbackup=10'
            - 'audit-log-maxsize=100'
          kube-apiserver-arg:
            - "oidc-issuer-url=https://accounts.google.com"
            - "oidc-client-id={{ google_oidc_client_id }}"
            - "oidc-username-claim=email"
            - "oidc-username-prefix="
          kube-controller-manager-arg:
            - 'terminated-pod-gc-threshold=10'
          kubelet-arg:
            - 'streaming-connection-idle-timeout=5m'
            - "tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
            - "pod-max-pids=1000"
            - "node-ip={{ ansible_default_ipv4.address }}"
          embedded-registry: true
        dest: /etc/rancher/k3s/config.yaml
      notify:
        - Restart k3s
    - name: Put registries.yaml config
      copy:
        content: |-
          mirrors:
            "*":
        dest: /etc/rancher/k3s/registries.yaml
      notify:
        - Restart k3s
  handlers:
    - name: Restart k3s
      shell: 'sudo systemctl restart k3s 2>/dev/null || echo "skipping"'

- name: Install k3s bootstrap server
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Ping host
      ping:
    - name: Install k3s bootstrap server
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
          INSTALL_K3S_EXEC="server --cluster-init"  \ 
          INSTALL_K3S_VERSION={{ k3s_version }} \
          K3S_NODE_NAME={{ inventory_hostname }} \
          sh -s -
    - name: Extract K3S_TOKEN from server output
      command: cat /var/lib/rancher/k3s/server/node-token
      register: k3s_token
      failed_when: k3s_token.failed or k3s_token.stdout is undefined
    - name: Set K3S_TOKEN as a fact
      set_fact:
        k3s_token: "{{ k3s_token.stdout }}"

- name: Install k3s servers
  become: true
  hosts: masters
  tasks:
    - name: Install k3s servers
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
        INSTALL_K3S_EXEC="server" \
        INSTALL_K3S_VERSION={{ k3s_version }} \
        K3S_URL=https://{{ hostvars['blade001']['ansible_default_ipv4'].address }}:6443 \
        K3S_TOKEN={{ hostvars['blade001']['k3s_token'] }} \
        K3S_NODE_NAME={{ inventory_hostname }} \
        sh -s -

- name: Harden security
  become: true
  hosts: [bootstrapMaster, masters]
  tasks:
    - name: Switch certificate permissions
      command: chmod -R 600 /var/lib/rancher/k3s/server/tls

- name: Install k3s workers
  become: true
  hosts: workers
  tasks:
    - name: Install k3s workers
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
        INSTALL_K3S_EXEC="agent --node-external-ip={{ ansible_default_ipv4.address }}" \
        INSTALL_K3S_VERSION={{ k3s_version }} \
        K3S_URL=https://{{ hostvars['blade001']['ansible_default_ipv4'].address }}:6443 \
        K3S_TOKEN={{ hostvars['blade001']['k3s_token'] }} \
        K3S_NODE_NAME={{ inventory_hostname }} \
        sh -

- name: Install CLI tools
  become: true
  hosts: pies
  tasks:
    - name: Check if kubectl exists
      stat:
        path: /usr/local/bin/kubectl
      register: kubectlExists
    - name: Install kubectl
      when: "not kubectlExists.stat.exists"
      shell: |-
        set -euxo pipefail
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
        rm kubectl
    - name: Check if helm exists
      stat:
        path: /usr/bin/helm
      register: helmExists
    - name: Install helm
      when: "not helmExists.stat.exists"
      shell: |-
        set -euxo pipefail
        curl https://get.helm.sh/helm-3.13.0-linux-arm64.tar.gz -o helm.tar.gz
        tar -zxvf helm.tar.gz
        sudo mv linux-arm64/helm /usr/local/bin/helm
        rm -rf linux-arm64 helm.tar.gz
    - name: Check if cilium exists
      stat:
        path: /usr/local/bin/cilium
      register: ciliumExists
    - name: Install cilium CLI
      when: "not ciliumExists.stat.exists"
      shell: |-
        set -euxo pipefail
        CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
        CLI_ARCH=amd64
        if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
        curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz
        curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
        sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
        sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
        rm cilium-linux-${CLI_ARCH}.tar.gz
        rm cilium-linux-${CLI_ARCH}.tar.gz.sha256sum

- name: Install cilium
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Install cilium
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        helm repo add cilium https://helm.cilium.io/
        helm repo update
        helm upgrade --install cilium cilium/cilium --version 1.17.4 \
          --set ipam.operator.clusterPoolIPv4PodCIDRList="10.42.0.0/16" \
          --namespace kube-system
        
        cilium status --wait
        
        kubectl get pods -n kube-system

        kubectl wait pods --all -n kube-system --for=condition=Ready --timeout=60m

- name: Configure Google OIDC RBAC
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Create ClusterRoleBinding for Google admin user
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        kubectl apply -f - <<'EOF'
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: google-admin-alice
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
          - kind: User
            name: {{ google_oidc_admin_email }}
            apiGroup: rbac.authorization.k8s.io
        EOF

# language: yaml
- name: Export admin kubeconfig and generate OIDC-only kubeconfig
  hosts: bootstrapMaster
  become: true
  gather_facts: true
  vars:
    control_machine_home: "{{ lookup('env', 'HOME') }}"
    kube_dest_dir: "{{ control_machine_home }}/.kube"
    kube_dest_file: "{{ control_machine_home }}/.kube/config"
    oauth_dest: "{{ control_machine_home }}/.kube/config-homelab"
    oidc_issuer: https://accounts.google.com
    oidc_user_name: google-oidc
    oidc_context_name: homelab
    external_api_server: "https://kubernetes.homelab.int.{{ domain }}:6443"

  tasks:
    - name: Read admin kubeconfig from k3s
      slurp:
        src: /etc/rancher/k3s/k3s.yaml
      register: admin_kube_slurp
      changed_when: false

    - name: Parse admin kubeconfig YAML
      set_fact:
        admin_kube: "{{ (admin_kube_slurp.content | b64decode) | from_yaml }}"

    - name: Ensure local kube dir exists on control machine
      file:
        path: "{{ kube_dest_dir }}"
        state: directory
        mode: '0700'
      delegate_to: localhost
      become: false

    - name: Write full-access admin kubeconfig to `{{ kube_dest_file }}`
      copy:
        content: "{{ admin_kube_slurp.content | b64decode }}"
        dest: "{{ kube_dest_file }}"
        mode: '0600'
      delegate_to: localhost
      become: false

    - name: Use OIDC client id from Ansible variable
      set_fact:
        oidc_client_id: "{{ google_oidc_client_id }}"
        oidc_client_secret: "{{ google_oidc_client_secret }}"
      when: google_oidc_client_id is defined and google_oidc_client_id != ''

    - name: Build OIDC-only kubeconfig dictionary
      set_fact:
        oauth_kubecfg_dict:
          apiVersion: v1
          kind: Config
          clusters:
            - name: "{{ admin_kube.clusters[0].name }}"
              cluster:
                server: "{{ external_api_server }}"
                certificate-authority-data: "{{ admin_kube.clusters[0].cluster['certificate-authority-data'] }}"
          users:
            - name: "{{ oidc_user_name }}"
              user:
                exec:
                  apiVersion: "client.authentication.k8s.io/v1"
                  command: kubectl
                  args:
                    - oidc-login
                    - get-token
                    - "--oidc-issuer-url={{ oidc_issuer }}"
                    - "--oidc-client-id={{ oidc_client_id }}"
                    - "--oidc-client-secret={{ oidc_client_secret }}"
                    - --oidc-extra-scope=openid
                    - --oidc-extra-scope=email
                    - --oidc-extra-scope=profile
                  interactiveMode: IfAvailable
          contexts:
            - name: "{{ oidc_context_name }}"
              context:
                cluster: "{{ admin_kube.clusters[0].name }}"
                user: "{{ oidc_user_name }}"
          current-context: "{{ oidc_context_name }}"
      when: oidc_client_id is defined and oidc_client_id != ''

    - name: Ensure .kube dir exists on control machine
      file:
        path: "{{ control_machine_home }}/.kube"
        state: directory
        mode: '0755'
      delegate_to: localhost
      become: false

    - name: Write OIDC-only kubeconfig to `{{ oauth_dest }}`
      copy:
        content: "{{ oauth_kubecfg_dict | to_nice_yaml }}"
        dest: "{{ oauth_dest }}"
        mode: '0600'
      delegate_to: localhost
      become: false
      when: oidc_client_id is defined and oidc_client_id != ''
