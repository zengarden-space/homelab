- name: System setup
  become: true
  hosts: pies
  tasks:
    - name: Ping hosts
      ping:
    - name: Update apt packages
      command: sudo apt-get update
    - name: Check if kernel parameters set
      stat:
        path: /etc/sysctl.d/90-kubelet.conf
      register: kubeletConfExists
    - name: kubelet kernel parameters
      shell: |-
        set -euxo pipefail
        tee /etc/sysctl.d/90-kubelet.conf <<'EOF'
        vm.panic_on_oom=0
        vm.overcommit_memory=1
        kernel.panic=10
        kernel.panic_on_oops=1
        EOF
      when: "not kubeletConfExists.stat.exists"
    - name: Check if cgroups are enabled
      command: cat /boot/firmware/cmdline.txt
      register: cmdlineContent
    - name: Enable cgroups
      command: sed -i -e 's/$/ cgroup_memory=1 cgroup_enable=memory/' /boot/firmware/cmdline.txt
      when: "'cgroup_memory=1 cgroup_enable=memory' not in cmdlineContent.stdout"
      notify:
        - Restart pi
  handlers:
    - name: Restart pi
      reboot:

- name: Prep servers
  become: true
  hosts: [bootstrapMaster, masters]
  tasks:
    - name: Ensure directory /var/lib/rancher/k3s/server exists
      file:
        path: /var/lib/rancher/k3s/server
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Ensure directory /etc/rancher exists
      file:
        path: /etc/rancher
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Ensure directory /etc/rancher/k3s exists
      file:
        path: /etc/rancher/k3s
        state: directory
        mode: '0600'
        owner: root
        group: root
      notify:
        - Restart k3s
    - name: Put PSA policy
      copy:
        content: |-
          apiVersion: apiserver.config.k8s.io/v1
          kind: AdmissionConfiguration
          plugins:
          - name: PodSecurity
            configuration:
              apiVersion: pod-security.admission.config.k8s.io/v1beta1
              kind: PodSecurityConfiguration
              defaults:
                enforce: "restricted"
                enforce-version: "latest"
                audit: "restricted"
                audit-version: "latest"
                warn: "restricted"
                warn-version: "latest"
              exemptions:
                usernames: []
                runtimeClasses: []
                namespaces: [kube-system, rook-ceph, velero, metallb-system, victoria-metrics, gitea-runner]
          - name: EventRateLimit
            configuration:
              apiVersion: eventratelimit.admission.k8s.io/v1alpha1
              kind: Configuration
              limits:
                - type: Server
                  qps: 50
                  burst: 100
                - type: Namespace
                  qps: 10
                  burst: 20
                - type: User
                  qps: 20
                  burst: 30
        dest: /var/lib/rancher/k3s/server/psa.yaml
      notify:
        - Restart k3s
    - name: Put audit policy
      copy:
        content: |-
          apiVersion: audit.k8s.io/v1
          kind: Policy
          rules:
          - level: Metadata
        dest: /var/lib/rancher/k3s/server/audit.yaml
      notify:
        - Restart k3s
    - name: Put k3s config
      copy:
        content: |-
          # Data directory configuration (should use NVMe mount)
          data-dir: "/var/lib/rancher/k3s"
          write-kubeconfig-mode: "600"
          flannel-backend: "none"
          disable-network-policy: true
          disable:
            - servicelb
            - traefik
          cluster-cidr: "10.42.0.0/16"
          service-cidr: "10.43.0.0/16"
          # TLS Configuration - Include node IPs and service IPs in certificates
          tls-san:
            - "{{ ansible_default_ipv4.address }}"
            - "{{ inventory_hostname }}"
            - "{{ inventory_hostname }}.{{ domain }}"
            - "10.43.0.1"  # Kubernetes service IP
            - "127.0.0.1"  # Localhost
            - "localhost"
            - "kubernetes"
            - "kubernetes.default"
            - "kubernetes.default.svc"
            - "kubernetes.default.svc.cluster.local"
            - "kubernetes.{{ domain }}"  # External API endpoint
          node-external-ip: "{{ ansible_default_ipv4.address }}"
          # Hardened security
          protect-kernel-defaults: true
          secrets-encryption: true
          kube-apiserver-arg:
            - "enable-admission-plugins=NodeRestriction,EventRateLimit"
            - 'admission-control-config-file=/var/lib/rancher/k3s/server/psa.yaml'
            - 'audit-log-path=/var/lib/rancher/k3s/server/logs/audit.log'
            - 'audit-policy-file=/var/lib/rancher/k3s/server/audit.yaml'
            - 'audit-log-maxage=30'
            - 'audit-log-maxbackup=10'
            - 'audit-log-maxsize=100'
          kube-controller-manager-arg:
            - 'terminated-pod-gc-threshold=10'
          kubelet-arg:
            - 'streaming-connection-idle-timeout=5m'
            - "tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
            - "pod-max-pids=1000"
            - "node-ip={{ ansible_default_ipv4.address }}"
          embedded-registry: true
        dest: /etc/rancher/k3s/config.yaml
      notify:
        - Restart k3s
    - name: Put registries.yaml config
      copy:
        content: |-
          mirrors:
            "*":
        dest: /etc/rancher/k3s/registries.yaml
      notify:
        - Restart k3s
  handlers:
    - name: Restart k3s
      shell: 'sudo systemctl restart k3s 2>/dev/null || echo "skipping"'

- name: Install k3s bootstrap server
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Ping host
      ping:
    - name: Install k3s bootstrap server
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
          INSTALL_K3S_EXEC="server --cluster-init"  \ 
          INSTALL_K3S_VERSION={{ k3s_version }} \
          K3S_NODE_NAME={{ inventory_hostname }} \
          sh -s -
    - name: Extract K3S_TOKEN from server output
      command: cat /var/lib/rancher/k3s/server/node-token
      register: k3s_token
      failed_when: k3s_token.failed or k3s_token.stdout is undefined
    - name: Set K3S_TOKEN as a fact
      set_fact:
        k3s_token: "{{ k3s_token.stdout }}"

- name: Install k3s servers
  become: true
  hosts: masters
  tasks:
    - name: Install k3s servers
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
        INSTALL_K3S_EXEC="server" \
        INSTALL_K3S_VERSION={{ k3s_version }} \
        K3S_URL=https://{{ hostvars['blade001']['ansible_default_ipv4'].address }}:6443 \
        K3S_TOKEN={{ hostvars['blade001']['k3s_token'] }} \
        K3S_NODE_NAME={{ inventory_hostname }} \
        sh -s -

- name: Harden security
  become: true
  hosts: [bootstrapMaster, masters]
  tasks:
    - name: Switch certificate permissions
      command: chmod -R 600 /var/lib/rancher/k3s/server/tls

- name: Install k3s workers
  become: true
  hosts: workers
  tasks:
    - name: Install k3s workers
      shell: |-
        set -euxo pipefail
        curl -sfL https://get.k3s.io |\
        INSTALL_K3S_EXEC="agent --node-external-ip={{ ansible_default_ipv4.address }}" \
        INSTALL_K3S_VERSION={{ k3s_version }} \
        K3S_URL=https://{{ hostvars['blade001']['ansible_default_ipv4'].address }}:6443 \
        K3S_TOKEN={{ hostvars['blade001']['k3s_token'] }} \
        K3S_NODE_NAME={{ inventory_hostname }} \
        sh -

- name: Install CLI tools
  become: true
  hosts: pies
  tasks:
    - name: Check if kubectl exists
      stat:
        path: /usr/local/bin/kubectl
      register: kubectlExists
    - name: Install kubectl
      when: "not kubectlExists.stat.exists"
      shell: |-
        set -euxo pipefail
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl"
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
        rm kubectl
    - name: Check if helm exists
      stat:
        path: /usr/bin/helm
      register: helmExists
    - name: Install helm
      when: "not helmExists.stat.exists"
      shell: |-
        set -euxo pipefail
        curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
        sudo apt-get install apt-transport-https --yes
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
        sudo apt-get update
        sudo apt-get install helm
    - name: Check if cilium exists
      stat:
        path: /usr/local/bin/cilium
      register: ciliumExists
    - name: Install cilium CLI
      when: "not ciliumExists.stat.exists"
      shell: |-
        set -euxo pipefail
        CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
        CLI_ARCH=amd64
        if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
        curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz
        curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
        sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
        sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
        rm cilium-linux-${CLI_ARCH}.tar.gz
        rm cilium-linux-${CLI_ARCH}.tar.gz.sha256sum

- name: Install cilium
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Install cilium
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        helm repo add cilium https://helm.cilium.io/
        helm repo update
        helm upgrade --install cilium cilium/cilium --version 1.17.4 \
          --namespace kube-system
        
        cilium status --wait
        
        kubectl get pods -n kube-system

        kubectl wait pods --all -n kube-system --for=condition=Ready --timeout=15m

- name: Deploy native Ceph cluster via Rook
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Install rook Helm repo
      shell: |-
        set -euxo pipefail
        helm repo add rook-release https://charts.rook.io/release
        helm repo update

    - name: Install rook-ceph operator
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        helm upgrade --install rook rook-release/rook-ceph --namespace rook-ceph --create-namespace \
          --set crds.enabled=true
        kubectl wait --for=condition=Available --timeout=5m -n rook-ceph deployment/rook-ceph-operator

- name: Prepare Ceph storage devices on all nodes (partitioned)
  become: true
  hosts: [bootstrapMaster, masters, workers]
  tasks:
    - name: Check NVMe Ceph partition exists
      stat:
        path: /dev/nvme0n1p2
      register: nvme_ceph_partition

    - name: Fail if Ceph partition not found
      fail:
        msg: "Ceph partition /dev/nvme0n1p2 not found. Run nvme-partitioning playbook first."
      when: not nvme_ceph_partition.stat.exists

    - name: Verify Ceph partition is clean
      shell: |
        echo "=== Ceph partition status ==="
        lsblk /dev/nvme0n1p2
        echo ""
        echo "=== Filesystem check ==="
        blkid /dev/nvme0n1p2 || echo "No filesystem detected (good for Ceph)"
      register: partition_status

    - name: Show partition status
      debug:
        msg: "{{ partition_status.stdout_lines }}"

- name: Deploy native Ceph cluster via Rook
  become: true
  hosts: bootstrapMaster
  tasks:
    - name: Create native CephCluster
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        cat <<EOF | kubectl apply -f -
        apiVersion: ceph.rook.io/v1
        kind: CephCluster
        metadata:
          name: rook-ceph
          namespace: rook-ceph
        spec:
          cephVersion:
            image: quay.io/ceph/ceph:v18.2.0
          dataDirHostPath: /var/lib/rook
          skipUpgradeChecks: false
          continueUpgradeAfterChecksEvenIfNotHealthy: false
          waitTimeoutForHealthyOSDInMinutes: 10
          mon:
            count: 3
            allowMultiplePerNode: false
          mgr:
            count: 2
            allowMultiplePerNode: false
            modules:
            - name: pg_autoscaler
              enabled: true
          dashboard:
            enabled: true
            port: 8443
            ssl: true
          monitoring:
            enabled: false
          network:
            requireMsgr2: false
          crashCollector:
            disable: false
          logCollector:
            enabled: true
            periodicity: daily
            maxLogSize: 500M
          cleanupPolicy:
            confirmation: ""
            sanitizeDisks:
              method: quick
              dataSource: zero
              iteration: 1
            allowUninstallWithVolumes: false
          placement:
            all:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - blade001
                      - blade002
                      - blade003
                      - blade004
                      - blade005
              tolerations:
              - effect: NoSchedule
                key: node.kubernetes.io/unschedulable
                operator: Exists
              - effect: NoSchedule
                key: node.cloudprovider.kubernetes.io/uninitialized
                operator: Exists
          annotations:
          labels:
          resources:
          removeOSDsIfOutAndSafeToRemove: false
          storage:
            useAllNodes: false
            useAllDevices: false
            config:
              osdsPerDevice: "1"
              encryptedDevice: "false"
              databaseSizeMB: "1024"
              walSizeMB: "1024"
            nodes:
            - name: "blade001"
              devices:
              - name: "/dev/nvme0n1p2"
                config:
                  osdsPerDevice: "1"
            - name: "blade002" 
              devices:
              - name: "/dev/nvme0n1p2"
                config:
                  osdsPerDevice: "1"
            - name: "blade003"
              devices:
              - name: "/dev/nvme0n1p2"
                config:
                  osdsPerDevice: "1"
            - name: "blade004"
              devices:
              - name: "/dev/nvme0n1p2"
                config:
                  osdsPerDevice: "1"
            - name: "blade005"
              devices:
              - name: "/dev/nvme0n1p2"
                config:
                  osdsPerDevice: "1"
          disruptionManagement:
            managePodBudgets: false
            osdMaintenanceTimeout: 30
            pgHealthCheckTimeout: 0
        EOF
        
        echo "⏳ Waiting for CephCluster to be ready..."
        kubectl wait --for=condition=Ready --timeout=10m -n rook-ceph cephcluster/rook-ceph

    - name: Verify Ceph OSDs are created
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🔍 Checking Ceph cluster status..."
        
        # Wait for OSDs to be created
        echo "⏳ Waiting for OSDs to be ready..."
        for i in {1..30}; do
          OSD_COUNT=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd --no-headers 2>/dev/null | wc -l)
          if [ "$OSD_COUNT" -ge 5 ]; then
            echo "✅ Found $OSD_COUNT OSDs"
            break
          fi
          echo "   Waiting for OSDs... ($OSD_COUNT/5 found)"
          sleep 10
        done
        
        # Show OSD status
        kubectl get pods -n rook-ceph -l app=rook-ceph-osd
        
        # Show Ceph cluster status via toolbox
        kubectl exec -n rook-ceph deployment/rook-ceph-tools -- ceph status || echo "Ceph tools not ready yet"
      ignore_errors: true

    - name: Deploy Ceph toolbox for debugging
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Deploy Ceph toolbox
        cat <<EOF | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: rook-ceph-tools
          namespace: rook-ceph
          labels:
            app: rook-ceph-tools
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: rook-ceph-tools
          template:
            metadata:
              labels:
                app: rook-ceph-tools
            spec:
              dnsPolicy: ClusterFirstWithHostNet
              containers:
              - name: rook-ceph-tools
                image: quay.io/ceph/ceph:v18.2.0
                command:
                  - /bin/bash
                  - -c
                  - |
                    # Wait for the connection and then maintain it
                    sleep infinity
                imagePullPolicy: IfNotPresent
                env:
                  - name: ROOK_CEPH_USERNAME
                    valueFrom:
                      secretKeyRef:
                        name: rook-ceph-mon
                        key: ceph-username
                  - name: ROOK_CEPH_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: rook-ceph-mon
                        key: ceph-secret
                volumeMounts:
                  - mountPath: /etc/ceph
                    name: ceph-config
                    readOnly: true
                  - name: mon-endpoint-volume
                    mountPath: /etc/rook
                    readOnly: true
              volumes:
                - name: ceph-config
                  configMap:
                    name: rook-ceph-config
                - name: mon-endpoint-volume
                  configMap:
                    name: rook-ceph-mon-endpoints
                    items:
                    - key: data
                      path: mon-endpoints
              tolerations:
                - key: node.kubernetes.io/unreachable
                  operator: Exists
                  effect: NoExecute
                  tolerationSeconds: 5
        EOF
        
        echo "⏳ Waiting for Ceph toolbox to be ready..."
        kubectl wait --for=condition=Available --timeout=5m -n rook-ceph deployment/rook-ceph-tools || true

    - name: Create RBD StorageClass and set as default
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Wait for Ceph cluster to be ready first
        echo "⏳ Waiting for Ceph cluster to be healthy..."
        kubectl wait --for=condition=Ready --timeout=15m -n rook-ceph cephcluster/rook-ceph || true
        
        # Create RBD pool
        cat <<EOF | kubectl apply -f -
        apiVersion: ceph.rook.io/v1
        kind: CephBlockPool
        metadata:
          name: replicapool
          namespace: rook-ceph
        spec:
          failureDomain: host
          replicated:
            size: 3
        EOF
        
        # Wait for pool to be ready
        kubectl wait --for=condition=Ready --timeout=5m -n rook-ceph cephblockpool/replicapool
        
        # Remove default annotation from local-path StorageClass
        kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' || true
        
        # Create RBD StorageClass and set as default
        cat <<EOF | kubectl apply -f -
        apiVersion: storage.k8s.io/v1
        kind: StorageClass
        metadata:
          name: ceph-rbd
          annotations:
            storageclass.kubernetes.io/is-default-class: "true"
        provisioner: rook-ceph.rbd.csi.ceph.com
        parameters:
          clusterID: rook-ceph
          pool: replicapool
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
        allowVolumeExpansion: true
        reclaimPolicy: Delete
        EOF

    - name: Create test PVC to validate Ceph integration
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Create test PVC
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: test-ceph-pvc
          namespace: default
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
          storageClassName: ceph-rbd
        EOF
        
        # Wait for PVC to be bound
        echo "Waiting for test PVC to be bound..."
        kubectl wait --for=condition=Bound pvc/test-ceph-pvc -n default --timeout=5m
        
        # Show PVC status
        kubectl get pvc test-ceph-pvc -n default
        
        # Show PV details
        PV_NAME=$(kubectl get pvc test-ceph-pvc -n default -o jsonpath='{.spec.volumeName}')
        kubectl get pv $PV_NAME
        
        echo "✅ Native Ceph integration test successful!"
      ignore_errors: true

    - name: Clean up test PVC
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        # Delete test PVC after successful validation
        kubectl delete pvc test-ceph-pvc -n default --ignore-not-found=true
        echo "🧹 Test PVC cleaned up"
      ignore_errors: true

    - name: Display Ceph cluster summary
      debug:
        msg:
          - "=================================================="
          - "🎉 Native Ceph Cluster Deployed Successfully"
          - "=================================================="
          - ""
          - "✅ Rook operator installed and configured"
          - "✅ Native Ceph cluster deployed on NVMe drives"
          - "✅ StorageClass 'ceph-rbd' created and set as default"
          - "✅ local-path StorageClass no longer default"
          - "✅ Test PVC validation completed"
          - ""
          - "📋 Cluster Details:"
          - "  • Monitors: 3 (on blade001, blade002, blade003)"
          - "  • Managers: 2"
          - "  • OSDs: 5 (NVMe partition 2 on all nodes)"
          - "  • Dashboard: Enabled on port 8443"
          - ""
          - "📊 Storage Configuration:"
          - "  • Pool: replicapool (3 replicas)"
          - "  • Features: layering"
          - "  • Reclaim Policy: Delete"
          - "  • Volume Expansion: Enabled"
          - "  • Default StorageClass: ceph-rbd"
          - ""
          - "🔧 Management Commands:"
          - "  • Cluster status: kubectl get cephcluster -n rook-ceph"
          - "  • Ceph status: kubectl exec -n rook-ceph deployment/rook-ceph-tools -- ceph status"
          - "  • Dashboard: kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard"

    - name: Final validation of Ceph cluster
      shell: |-
        set -euxo pipefail
        export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
        
        echo "🔍 Performing final validation..."
        
        # Check Rook operator status
        OPERATOR_READY=$(kubectl get deployment rook-ceph-operator -n rook-ceph -o jsonpath='{.status.readyReplicas}')
        echo "✅ Rook operator ready replicas: ${OPERATOR_READY:-0}"
        
        # Check CephCluster status
        CLUSTER_PHASE=$(kubectl get cephcluster rook-ceph -n rook-ceph -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        echo "✅ CephCluster phase: ${CLUSTER_PHASE}"
        
        # Check StorageClass
        DEFAULT_SC=$(kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}')
        echo "✅ Default StorageClass: ${DEFAULT_SC}"
        
        # Check CSI driver pods
        CSI_PODS=$(kubectl get pods -n rook-ceph -l app=csi-rbdplugin --no-headers 2>/dev/null | wc -l)
        echo "✅ CSI RBD driver pods: ${CSI_PODS}"
        
        # Check OSDs
        OSD_PODS=$(kubectl get pods -n rook-ceph -l app=rook-ceph-osd --no-headers 2>/dev/null | wc -l)
        echo "✅ Ceph OSD pods: ${OSD_PODS}"
        
        echo "🎉 Native Ceph cluster validation completed!"
      ignore_errors: true
